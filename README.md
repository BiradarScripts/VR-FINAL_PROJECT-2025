# üöÄ Multimodal Visual Question Answering with Amazon Berkeley Objects (ABO) Dataset

## Project Overview

> This project focuses on developing a **Multimodal Visual Question Answering (VQA)** system using the rich **Amazon Berkeley Objects (ABO)** dataset. Our objective is to create a multiple-choice VQA dataset from ABO, establish and evaluate robust baseline models, and then significantly improve their performance through fine-tuning using the **Low-Rank Adaptation (LoRA)** technique. The ultimate goal is to build a high-performing VQA model capable of answering questions about visual data, which will be rigorously evaluated on a hidden dataset and an undisclosed metric.

---

## üéØ Introduction

Visual Question Answering (VQA) is a challenging multimodal task that requires models to understand both visual content (images) and natural language questions, then provide accurate answers. This project addresses the complexities of VQA by leveraging the diverse and detailed Amazon Berkeley Objects (ABO) dataset. We aim to create a practical multiple-choice VQA dataset, evaluate existing models, and innovate by applying LoRA for efficient and effective fine-tuning on resource-constrained environments.

---

## üåü Project Goals

Our primary objectives for this project are:

* **Dataset Generation:** Construct a comprehensive multiple-choice VQA dataset from the raw ABO data.
* **Baseline Establishment:** Evaluate the performance of established VQA models as baselines.
* **Efficient Fine-tuning:** Implement and apply **LoRA** for efficient fine-tuning of large pre-trained models.
* **Performance Optimization:** Achieve state-of-the-art or near state-of-the-art performance on the VQA task within given constraints.
* **Robust Evaluation:** Assess model performance using standard VQA metrics and prepare for evaluation on hidden test sets.

---

## üì¶ Dataset: Amazon Berkeley Objects (ABO)

The **Amazon Berkeley Objects (ABO)** dataset is a rich collection of product images and associated metadata. It features diverse objects, viewpoints, and lighting conditions, making it an ideal candidate for developing robust VQA models. For this project, we will be extracting relevant information from the ABO dataset to formulate questions and generate multiple-choice answer options.

* **Dataset Size (approx.):** [Provide approximate number of images/objects if known after initial exploration, e.g., "Millions of images across thousands of objects."]
* **Data Modalities:** Images, 3D assets, metadata (e.g., product descriptions, attributes).
* **Expected VQA Task:** Given an image from ABO and a natural language question about it, select the correct answer from a set of multiple choices.

---

## üßë‚Äçüíª Team Members

| Name              | Student ID    
| :---------------- | :------------ 
| Aryaman Pathak    | IMT2022513 
| Rutul Patel       | IMT2022021   
| Shreyas Biradar   | IMT2022529   

---

## üôè Acknowledgments

This project is undertaken as part of the **[Course Name/Code, e.g., "Multimodal Machine Learning (CS XXXX)"]** course at **[Your University Name]**. We extend our gratitude to the course instructors and teaching assistants for their guidance and the opportunity to work on this exciting project.

---

## üìÑ License

This project is licensed under the **[Choose a License, e.g., MIT License]** - see the `LICENSE` file for details.

---
