# Multimodal Visual Question Answering with Amazon Berkeley Objects (ABO) Dataset

## Project Overview

Using the extensive Amazon Berkeley Objects (ABO) dataset, this project focuses on creating a "Multimodal Visual Question Answering" (VQA) system.  We want to use the **Low-Rank Adaptation (LoRA)** method to create a multiple-choice VQA dataset from ABO, establish and evaluate robust baseline models, and significantly enhance their performance by fine-tuning them. The ultimate goal is to build a high-performing VQA model capable of answering questions about visual data, which will be rigorously evaluated on a hidden dataset and an undisclosed metric.


##  Team Members

| Name              | Student ID    
| :---------------- | :------------ 
| Aryaman Pathak    | IMT2022513 
| Rutul Patel       | IMT2022021   
| Shreyas Biradar   | IMT2022529   

---

##  Acknowledgments

This project is undertaken as part of the **[Course Name/Code, e.g., "Multimodal Machine Learning (CS XXXX)"]** course at **[Your University Name]**. We extend our gratitude to the course instructors and teaching assistants for their guidance and the opportunity to work on this exciting project.

---

##  License

This project is licensed under the **[Choose a License, e.g., MIT License]** - see the `LICENSE` file for details.

---
