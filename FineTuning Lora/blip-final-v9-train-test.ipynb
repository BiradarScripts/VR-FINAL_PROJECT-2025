{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11645228,"datasetId":7307471,"databundleVersionId":12113438},{"sourceType":"datasetVersion","sourceId":11629593,"datasetId":7296443,"databundleVersionId":12095300},{"sourceType":"datasetVersion","sourceId":11757478,"datasetId":7359196,"databundleVersionId":12240513},{"sourceType":"datasetVersion","sourceId":11676506,"datasetId":7328450,"databundleVersionId":12148949},{"sourceType":"datasetVersion","sourceId":11676725,"datasetId":7328608,"databundleVersionId":12149207}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft torch accelerate pillow bitsandbytes # bitsandbytes often needed for efficient loading","metadata":{"execution":{"iopub.status.busy":"2025-05-10T05:06:41.706143Z","iopub.execute_input":"2025-05-10T05:06:41.706485Z","iopub.status.idle":"2025-05-10T05:07:57.514333Z","shell.execute_reply.started":"2025-05-10T05:06:41.706467Z","shell.execute_reply":"2025-05-10T05:07:57.513417Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m623.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import Blip2ForConditionalGeneration, Blip2Processor, TrainingArguments, Trainer\nfrom peft import get_peft_model, LoraConfig, TaskType","metadata":{"execution":{"iopub.status.busy":"2025-05-10T09:36:43.622474Z","iopub.execute_input":"2025-05-10T09:36:43.623515Z","iopub.status.idle":"2025-05-10T09:36:43.627263Z","shell.execute_reply.started":"2025-05-10T09:36:43.623486Z","shell.execute_reply":"2025-05-10T09:36:43.626565Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, json_dirs, base_img_path, processor):\n        self.samples = []\n        self.processor = processor\n        self.base_img_path = base_img_path\n\n        print(f\"📁 Scanning JSON folders: {json_dirs}\")\n\n        for dir_path in ([json_dirs] if isinstance(json_dirs, str) else json_dirs):\n            for root, _, files in os.walk(dir_path):\n                for file in files:\n                    if file.endswith(\".json\"):\n                        json_path = os.path.join(root, file)\n                        with open(json_path, \"r\") as f:\n                            data = json.load(f)\n\n                        image_rel_path = data[\"image_path\"].replace(\"\\\\\", \"/\")\n\n                        # Remove everything before and including 'abo-images-small/'\n                        if \"abo-images-small/\" in image_rel_path:\n                            image_rel_path = image_rel_path.split(\"abo-images-small/\", 1)[1]\n\n                        image_path = os.path.join(base_img_path, image_rel_path)\n\n                        for qa in data[\"qa_pairs\"]:\n                            self.samples.append({\n                                \"image_path\": image_path,\n                                \"question\": qa[\"question\"],\n                                \"answer\": qa[\"answer\"]\n                            })\n\n        print(f\"✅ Loaded {len(self.samples)} samples.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n\n        inputs = self.processor(\n            images=image,\n            text=sample[\"question\"],\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128\n        )\n\n        labels = self.processor.tokenizer(\n            sample[\"answer\"],\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=10\n        ).input_ids\n\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs[\"labels\"] = labels.squeeze(0)\n\n        # Remove inputs_embeds if it exists\n        if \"inputs_embeds\" in inputs:\n            del inputs[\"inputs_embeds\"]\n\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2025-05-10T09:36:51.145717Z","iopub.execute_input":"2025-05-10T09:36:51.146239Z","iopub.status.idle":"2025-05-10T09:36:51.154669Z","shell.execute_reply.started":"2025-05-10T09:36:51.146206Z","shell.execute_reply":"2025-05-10T09:36:51.153910Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_metrics(pred):\n    preds = pred.predictions\n    labels = pred.label_ids\n\n    tokenizer = Blip2Processor.from_pretrained(\"Salesforce/blip-vqa-base\").tokenizer\n    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    correct = sum(p.strip().lower() == l.strip().lower() for p, l in zip(pred_texts, label_texts))\n    acc = correct / len(label_texts)\n    print(f\"✅ Accuracy: {acc:.4f}\")\n    return {\"accuracy\": acc}","metadata":{"execution":{"iopub.status.busy":"2025-05-10T09:36:54.742502Z","iopub.execute_input":"2025-05-10T09:36:54.742774Z","iopub.status.idle":"2025-05-10T09:36:54.747926Z","shell.execute_reply.started":"2025-05-10T09:36:54.742754Z","shell.execute_reply":"2025-05-10T09:36:54.747225Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering, TrainingArguments, Trainer\nimport torch\nfrom torch.utils.data import random_split\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2025-05-10T05:09:26.177996Z","iopub.execute_input":"2025-05-10T05:09:26.178639Z","iopub.status.idle":"2025-05-10T05:09:26.213093Z","shell.execute_reply.started":"2025-05-10T05:09:26.178618Z","shell.execute_reply":"2025-05-10T05:09:26.212378Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device=\"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2025-05-10T05:09:31.458072Z","iopub.execute_input":"2025-05-10T05:09:31.458831Z","iopub.status.idle":"2025-05-10T05:09:31.462507Z","shell.execute_reply.started":"2025-05-10T05:09:31.458793Z","shell.execute_reply":"2025-05-10T05:09:31.461719Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Training cell \n","metadata":{}},{"cell_type":"code","source":"\ntorch.cuda.empty_cache()  # Clears unused memory from the cache","metadata":{"execution":{"iopub.status.busy":"2025-05-10T05:09:36.298297Z","iopub.execute_input":"2025-05-10T05:09:36.299044Z","iopub.status.idle":"2025-05-10T05:09:36.302325Z","shell.execute_reply.started":"2025-05-10T05:09:36.299018Z","shell.execute_reply":"2025-05-10T05:09:36.301673Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# from transformers import BlipProcessor, BlipForQuestionAnswering\n# from peft import get_peft_model, LoraConfig, TaskType\n# from torch.utils.data import DataLoader, random_split\n# import torch\n# from torch import optim\n# import os\n# import matplotlib.pyplot as plt\n# import json\n\n# # ---------- Setup ----------\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Load processor and base model\n# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n# model.to(device)\n\n# # Apply LoRA\n# lora_config = LoraConfig(\n#     r=8,\n#     lora_alpha=32,\n#     target_modules=['qkv', 'projection'],\n#     lora_dropout=0.05,\n#     bias='none'\n# )\n# model = get_peft_model(model, lora_config)\n# model.print_trainable_parameters()\n\n# # ---------- Dataset Setup ----------\n# json_root_dir = \"/kaggle/input/master-train/master_train/batch_3\"\n# image_base_dir = \"/kaggle/input/abo-dataset\"\n# dataset = VQADataset(json_dirs=json_root_dir, base_img_path=image_base_dir, processor=processor)\n\n# # Split dataset\n# split_ratio = 0.9\n# train_size = int(split_ratio * len(dataset))\n# eval_size = len(dataset) - train_size\n# train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n\n# # Create DataLoaders\n# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n# eval_loader = DataLoader(eval_dataset, batch_size=8)\n\n","metadata":{"execution":{"execution_failed":"2025-05-10T04:39:24.462Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import torch\n# import torch.optim as optim\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n\n# # ---------- Optimizer ----------\n# optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\n# # Save setup\n# save_path = \"/kaggle/working/model_latest\"\n# os.makedirs(save_path, exist_ok=True)\n# save_every = 1000  # save every N steps\n\n# # Resume state (optional)\n# resume_state_path = os.path.join(save_path, \"training_state.pt\")\n# start_step = 0\n# loss_history = []\n\n# if os.path.exists(resume_state_path):\n#     print(\"🔁 Resuming training from checkpoint...\")\n#     checkpoint = torch.load(resume_state_path)\n#     model.load_state_dict(checkpoint[\"model_state\"])\n#     optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n#     start_step = checkpoint[\"step\"]\n#     loss_history = checkpoint[\"loss_history\"]\n\n# # ---------- Training Loop ----------\n# epochs = 1\n# model.train()\n# step_counter = start_step\n# total_steps = len(train_loader) * epochs\n\n# for epoch in range(epochs):\n#     print(f\"🔥 Epoch {epoch + 1}\")\n#     epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=False)\n#     for batch in epoch_bar:\n#         step_counter += 1\n\n#         # Move inputs to device\n#         inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n\n#         # Forward pass\n#         outputs = model(**inputs)\n#         loss = outputs.loss\n\n#         # Backward pass\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         # Log loss\n#         loss_history.append((step_counter, loss.item()))\n\n#         # Progress update on tqdm bar\n#         epoch_bar.set_postfix({\n#             \"Step\": step_counter,\n#             \"Loss\": f\"{loss.item():.4f}\",\n#             \"Progress\": f\"{(step_counter / total_steps) * 100:.2f}%\"\n#         })\n\n#         # Save every N steps\n#         if step_counter % save_every == 0:\n#             print(f\"💾 Saving at step {step_counter}...\")\n#             model.save_pretrained(save_path)\n#             processor.save_pretrained(save_path)\n#             torch.save({\n#                 \"model_state\": model.state_dict(),\n#                 \"optimizer_state\": optimizer.state_dict(),\n#                 \"step\": step_counter,\n#                 \"loss_history\": loss_history,\n#             }, resume_state_path)\n\n# # Final Save\n# print(\"✅ Final save after training...\")\n# model.save_pretrained(save_path)\n# processor.save_pretrained(save_path)\n# torch.save({\n#     \"model_state\": model.state_dict(),\n#     \"optimizer_state\": optimizer.state_dict(),\n#     \"step\": step_counter,\n#     \"loss_history\": loss_history,\n# }, resume_state_path)\n\n# # ---------- Plot Loss ----------\n# steps, losses = zip(*loss_history)\n# plt.figure(figsize=(10, 5))\n# plt.plot(steps, losses, label=\"Training Loss\")\n# plt.xlabel(\"Step\")\n# plt.ylabel(\"Loss\")\n# plt.title(\"Loss vs. Training Step\")\n# plt.grid(True)\n# plt.legend()\n# plt.savefig(os.path.join(save_path, \"loss_plot.png\"))\n# plt.show()\n","metadata":{"execution":{"execution_failed":"2025-05-10T04:39:24.462Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Batch -2 onwards training","metadata":{}},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader, random_split\nimport torch\nfrom torch import optim\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\n\n# ---------- Setup ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Paths\nload_path = \"/kaggle/input/blip-finetunedmodel-versions/model_latest_v8\" # Load from\nsave_path = \"/kaggle/working/model_latest_v9\"                # Save to\nresume_state_path = os.path.join(save_path, \"training_state.pt\")\n\n# Load processor and model from load_path\nprocessor = BlipProcessor.from_pretrained(load_path)\nmodel = BlipForQuestionAnswering.from_pretrained(load_path)\n\n# Apply LoRA configuration\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=['qkv', 'projection'],\n    lora_dropout=0.05,\n    bias='none'\n)\nmodel = get_peft_model(model, lora_config)\nmodel.to(device)\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=10e-5)\n\n# Load checkpoint if available\nif os.path.exists(resume_state_path):\n    print(\"🔁 Loading checkpoint...\")\n    checkpoint = torch.load(resume_state_path)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n    start_step = checkpoint[\"step\"]\n    loss_history = checkpoint[\"loss_history\"]\nelse:\n    print(\"🆕 Starting fresh training.\")\n    start_step = 0\n    loss_history = []\n\n# ---------- Dataset ----------\njson_root_dir = \"/kaggle/input/master-train/master_train/batch_9\"\nimage_base_dir = \"/kaggle/input/abo-dataset\"\ndataset = VQADataset(json_dirs=json_root_dir, base_img_path=image_base_dir, processor=processor)\n\n# Split dataset\nsplit_ratio = 0.9\ntrain_size = int(split_ratio * len(dataset))\neval_size = len(dataset) - train_size\ntrain_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\neval_loader = DataLoader(eval_dataset, batch_size=8)\n\n# ---------- Training ----------\nepochs = 1\nsave_every = 1000\nstep_counter = start_step\ntotal_steps = len(train_loader) * epochs\nmodel.train()\n\nfor epoch in range(epochs):\n    print(f\"\\n🔥 Epoch {epoch + 1}/{epochs}\")\n    pbar = tqdm(train_loader, total=len(train_loader), desc=f\"Training\")\n    for batch in pbar:\n        step_counter += 1\n\n        inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        outputs = model(**inputs)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_history.append((step_counter, loss.item()))\n\n        pbar.set_postfix({\"Step\": step_counter, \"Loss\": f\"{loss.item():.4f}\"})\n\n        # Periodic checkpoint saving\n        if step_counter % save_every == 0:\n            os.makedirs(save_path, exist_ok=True)\n            model.save_pretrained(save_path)\n            processor.save_pretrained(save_path)\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"step\": step_counter,\n                \"loss_history\": loss_history\n            }, resume_state_path)\n\n# ---------- Final Save ----------\nprint(\"✅ Final save after training.\")\nos.makedirs(save_path, exist_ok=True)\nmodel.save_pretrained(save_path)\nprocessor.save_pretrained(save_path)\ntorch.save({\n    \"model_state\": model.state_dict(),\n    \"optimizer_state\": optimizer.state_dict(),\n    \"step\": step_counter,\n    \"loss_history\": loss_history\n}, resume_state_path)\n\n# ---------- Loss Plot ----------\nsteps, losses = zip(*loss_history)\n# Extract model name from save_path\nmodel_name = os.path.basename(save_path)\n\n# Plot 1: Raw Loss\nplt.figure(figsize=(10, 5))\nplt.plot(steps, losses, label=\"Training Loss\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(f\"{model_name} - Training Loss vs. Steps\")\nplt.grid(True)\nplt.legend()\nplt.savefig(os.path.join(save_path, \"loss_plot_raw.png\"))\nplt.show()\n\n# Plot 2: Smoothed Loss\nsmoothed_losses = gaussian_filter1d(losses, sigma=5)\nplt.figure(figsize=(10, 5))\nplt.plot(steps, smoothed_losses, label=\"Smoothed Training Loss\", color='orange')\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(f\"{model_name} - Smoothed Loss vs. Steps\")\nplt.grid(True)\nplt.legend()\nplt.savefig(os.path.join(save_path, \"loss_plot_smoothed.png\"))\nplt.show()\n\n# Plot 3: Log-Scale Loss\nplt.figure(figsize=(10, 5))\nplt.semilogy(steps, losses, label=\"Log-Scale Training Loss\", color='green')\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss (log scale)\")\nplt.title(f\"{model_name} - Log-Scale Loss vs. Steps\")\nplt.grid(True, which='both')\nplt.legend()\nplt.savefig(os.path.join(save_path, \"loss_plot_logscale.png\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-10T05:09:43.703077Z","iopub.execute_input":"2025-05-10T05:09:43.703542Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3454bbe6ba3445ea0dff4b3cd154c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158038ac4a454204bc73ce05e7d78a34"}},"metadata":{}},{"name":"stdout","text":"🔁 Loading checkpoint...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3786007223.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(resume_state_path)\n","output_type":"stream"},{"name":"stdout","text":"📁 Scanning JSON folders: /kaggle/input/master-train/master_train/batch_9\n✅ Loaded 142089 samples.\n\n🔥 Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/15985 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nTraining:  74%|███████▍  | 11833/15985 [2:08:11<43:48,  1.58it/s, Step=12834, Loss=8.4139]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import shutil\n# Compress folder into a zip file\nshutil.make_archive('/kaggle/working/model_latest_v9', 'zip', '/kaggle/working/model_latest_v9')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert-score\n!pip install scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:32:32.144411Z","iopub.execute_input":"2025-05-10T09:32:32.144716Z","iopub.status.idle":"2025-05-10T09:33:48.026182Z","shell.execute_reply.started":"2025-05-10T09:32:32.144666Z","shell.execute_reply":"2025-05-10T09:33:48.025471Z"}},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Testing cell \n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.amp import autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom accelerate import Accelerator\nimport time\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score_fn\n\n# Step 1: Set environment variable to disable tokenizer parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize Accelerator\naccelerator = Accelerator()\ndevice = accelerator.device\n\n# Load model and processor\nload_path = \"/kaggle/working/model_latest_v9\"\nmodel = BlipForQuestionAnswering.from_pretrained(load_path)\nprocessor = BlipProcessor.from_pretrained(load_path)\n\n# Prepare model for distributed inference\nmodel = accelerator.prepare(model)\n\n# Load dataset\ntest_json_dir = \"/kaggle/input/master-test/test_dataset\"\ntest_image_dir = \"/kaggle/input/abo-dataset\"\ntest_dataset = VQADataset(\n    json_dirs=test_json_dir,\n    base_img_path=test_image_dir,\n    processor=processor\n)\n\n# DataLoader setup\nbatch_size = 100\nnum_workers = 14\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True,\n    persistent_workers=True\n)\n\n# Evaluation setup\nmodel.eval()\ncorrect = 0\ntotal = 0\nbatch_count = len(test_loader)\n\npredicted_all = []\ntrue_all = []\n\nstart_time = time.time()\n\n# Evaluation loop\nwith torch.no_grad():\n    for batch in tqdm(test_loader, total=batch_count, desc=\"Evaluating\"):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        answers = batch[\"labels\"]\n\n        with autocast(device_type=device.type, dtype=torch.float16):\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                max_length=20,\n                do_sample=False,\n                num_beams=1\n            )\n\n        predicted_answers = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        decoded_answers = [\n            processor.tokenizer.decode(ans, skip_special_tokens=True)\n            if isinstance(ans, torch.Tensor) else ans\n            for ans in answers\n        ]\n\n        for pred, true_ans_str in zip(predicted_answers, decoded_answers):\n            pred_clean = pred.strip().lower()\n            true_clean = true_ans_str.strip().lower()\n\n            predicted_all.append(pred_clean)\n            true_all.append(true_clean)\n\n            if pred_clean == true_clean:\n                correct += 1\n            total += 1\n\nend_time = time.time()\nelapsed = end_time - start_time\n\n# Compute Exact Match\nexact_match = 100 * correct / total\n\n# Compute BERTScore\nP, R, F1 = bert_score_fn(predicted_all, true_all, lang=\"en\", verbose=True)\nbert_score_f1 = F1.mean().item()\nbert_score_precision = P.mean().item()\nbert_score_recall = R.mean().item()\n\n# Print all metrics\nprint(f\"\\n📊 Evaluation Metrics:\")\nprint(f\"✅ Exact Match (EM): {exact_match:.2f}%\")\nprint(f\"🤖 BERTScore - Precision: {bert_score_precision:.4f}\")\nprint(f\"🤖 BERTScore - Recall:    {bert_score_recall:.4f}\")\nprint(f\"🤖 BERTScore - F1:        {bert_score_f1:.4f}\")\nprint(f\"\\n⏱️ Total Evaluation Time: {elapsed:.2f} seconds for {total} samples.\")\nprint(f\"⚡ Inference Speed: {total / elapsed:.2f} it/sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:37:02.392763Z","iopub.execute_input":"2025-05-10T09:37:02.393040Z","iopub.status.idle":"2025-05-10T14:02:22.325546Z","shell.execute_reply.started":"2025-05-10T09:37:02.393022Z","shell.execute_reply":"2025-05-10T14:02:22.324809Z"}},"outputs":[{"name":"stdout","text":"📁 Scanning JSON folders: /kaggle/input/master-test/test_dataset\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded 482036 samples.\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 4821/4821 [4:20:05<00:00,  3.24s/it]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce902f313ae84ebeb56cba8739e5f488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da3cc45a962450dae2322570d240146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51cb542d0dc74b7f9e45b016afa14683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65a5d8d2c3746f2a12eb5250f5eb868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627cbdbb8c4f4eecac2f18386fa03999"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e478701fc54a4f39bef00efa270dd1a1"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/429 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dc0f2583c048d4afd2b41deefdf2f2"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7532 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc1889d01bc465b9519227de67598fc"}},"metadata":{}},{"name":"stderr","text":"Warning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\n","output_type":"stream"},{"name":"stdout","text":"done in 92.60 seconds, 5205.33 sentences/sec\n\n📊 Evaluation Metrics:\n✅ Exact Match (EM): 19.79%\n🤖 BERTScore - Precision: 0.9573\n🤖 BERTScore - Recall:    0.9357\n🤖 BERTScore - F1:        0.9454\n\n⏱️ Total Evaluation Time: 15605.56 seconds for 482036 samples.\n⚡ Inference Speed: 30.89 it/sec\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(f\"✅ Exact Match (EM): {exact_match:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:05:28.981000Z","iopub.execute_input":"2025-05-10T14:05:28.981220Z","iopub.status.idle":"2025-05-10T14:05:28.989033Z","shell.execute_reply.started":"2025-05-10T14:05:28.981205Z","shell.execute_reply":"2025-05-10T14:05:28.988309Z"}},"outputs":[{"name":"stdout","text":"✅ Exact Match (EM): 19.79%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from bert_score import score as bert_score_fn\nP, R, F1 = bert_score_fn(predicted_all, true_all, lang=\"en\", verbose=True)\nbert_score_precision = P.mean().item()\nbert_score_recall = R.mean().item()\nbert_score_f1 = F1.mean().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:05:28.989803Z","iopub.execute_input":"2025-05-10T14:05:28.990037Z","iopub.status.idle":"2025-05-10T14:07:02.285698Z","shell.execute_reply.started":"2025-05-10T14:05:28.990018Z","shell.execute_reply":"2025-05-10T14:07:02.284963Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/429 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5077c13204243028a5944559d5ee9de"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7532 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6df787aa95489ba85cbad466a3ad9c"}},"metadata":{}},{"name":"stderr","text":"Warning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\nWarning: Empty reference sentence detected; setting raw BERTScores to 0.\n","output_type":"stream"},{"name":"stdout","text":"done in 92.02 seconds, 5238.10 sentences/sec\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(f\"🤖 BERTScore - Precision: {bert_score_precision:.4f}\")\nprint(f\"🤖 BERTScore - Recall:    {bert_score_recall:.4f}\")\nprint(f\"🤖 BERTScore - F1:        {bert_score_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:07:02.286537Z","iopub.execute_input":"2025-05-10T14:07:02.286792Z","iopub.status.idle":"2025-05-10T14:07:02.291373Z","shell.execute_reply.started":"2025-05-10T14:07:02.286775Z","shell.execute_reply":"2025-05-10T14:07:02.290647Z"}},"outputs":[{"name":"stdout","text":"🤖 BERTScore - Precision: 0.9573\n🤖 BERTScore - Recall:    0.9357\n🤖 BERTScore - F1:        0.9454\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(f\"\\n⏱️ Total Evaluation Time: {elapsed:.2f} seconds for {total} samples.\")\nprint(f\"⚡ Inference Speed: {total / elapsed:.2f} it/sec\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:07:02.292144Z","iopub.execute_input":"2025-05-10T14:07:02.292383Z","iopub.status.idle":"2025-05-10T14:07:02.354375Z","shell.execute_reply.started":"2025-05-10T14:07:02.292360Z","shell.execute_reply":"2025-05-10T14:07:02.353780Z"}},"outputs":[{"name":"stdout","text":"\n⏱️ Total Evaluation Time: 15605.56 seconds for 482036 samples.\n⚡ Inference Speed: 30.89 it/sec\n","output_type":"stream"}],"execution_count":21}]}