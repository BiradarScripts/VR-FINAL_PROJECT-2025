{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11472097,"sourceType":"datasetVersion","datasetId":7189537},{"sourceId":11650039,"sourceType":"datasetVersion","datasetId":7310940},{"sourceId":11676506,"sourceType":"datasetVersion","datasetId":7328450},{"sourceId":11688823,"sourceType":"datasetVersion","datasetId":7336457}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\nquestion = \"how many humans are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:42:53.539829Z","iopub.execute_input":"2025-05-07T07:42:53.540266Z","iopub.status.idle":"2025-05-07T07:42:58.984214Z","shell.execute_reply.started":"2025-05-07T07:42:53.540237Z","shell.execute_reply":"2025-05-07T07:42:58.983509Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:43:06.549514Z","iopub.execute_input":"2025-05-07T07:43:06.550162Z","iopub.status.idle":"2025-05-07T07:43:09.494863Z","shell.execute_reply.started":"2025-05-07T07:43:06.550139Z","shell.execute_reply":"2025-05-07T07:43:09.494121Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install bert-score rouge-score nltk sacrebleu --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:43:16.613690Z","iopub.execute_input":"2025-05-07T07:43:16.614060Z","iopub.status.idle":"2025-05-07T07:43:19.837144Z","shell.execute_reply.started":"2025-05-07T07:43:16.614035Z","shell.execute_reply":"2025-05-07T07:43:19.836332Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# !pip install git+https://github.com/neulab/BARTScore.git --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/neulab/BARTScore.git\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:43:23.684005Z","iopub.execute_input":"2025-05-07T07:43:23.684686Z","iopub.status.idle":"2025-05-07T07:43:23.848431Z","shell.execute_reply.started":"2025-05-07T07:43:23.684658Z","shell.execute_reply":"2025-05-07T07:43:23.847243Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'BARTScore' already exists and is not an empty directory.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"setup_code = \"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='bart_score',\n    version='0.1.0',\n    description='BARTScore: Evaluating Text Generation with BART',\n    author='Neulab',\n    url='https://github.com/neulab/BARTScore',\n    packages=find_packages(),\n    install_requires=[\n        'torch>=1.7.0',\n        'transformers>=4.0.0',\n        'scipy',\n        'numpy',\n        'tqdm',\n    ],\n    classifiers=[\n        'Programming Language :: Python :: 3',\n        'License :: OSI Approved :: MIT License',\n    ],\n    python_requires='>=3.6',\n)\n\"\"\"\nwith open(\"BARTScore/setup.py\", \"w\") as f:\n    f.write(setup_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:43:29.289828Z","iopub.execute_input":"2025-05-07T07:43:29.290189Z","iopub.status.idle":"2025-05-07T07:43:29.295570Z","shell.execute_reply.started":"2025-05-07T07:43:29.290160Z","shell.execute_reply":"2025-05-07T07:43:29.294850Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install ./BARTScore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:43:33.531586Z","iopub.execute_input":"2025-05-07T07:43:33.532291Z","iopub.status.idle":"2025-05-07T07:43:42.161580Z","shell.execute_reply.started":"2025-05-07T07:43:33.532268Z","shell.execute_reply":"2025-05-07T07:43:42.160805Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing ./BARTScore\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from bart_score==0.1.0) (2.5.1+cu124)\nRequirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from bart_score==0.1.0) (4.51.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from bart_score==0.1.0) (1.15.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bart_score==0.1.0) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from bart_score==0.1.0) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->bart_score==0.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->bart_score==0.1.0) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->bart_score==0.1.0) (0.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bart_score==0.1.0) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->bart_score==0.1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bart_score==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bart_score==0.1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bart_score==0.1.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bart_score==0.1.0) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->bart_score==0.1.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->bart_score==0.1.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->bart_score==0.1.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->bart_score==0.1.0) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bart_score==0.1.0) (2024.2.0)\nBuilding wheels for collected packages: bart_score\n  Building wheel for bart_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for bart_score: filename=bart_score-0.1.0-py3-none-any.whl size=5317 sha256=17596198578b8cd43ef40e8e83f697fa9ec149746abf7010e91c9b9f07e39b9f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0wosw4j5/wheels/98/01/d3/10d9ca6b2865ade955e5f3c526e4a9b480ab24cea364919527\nSuccessfully built bart_score\nInstalling collected packages: bart_score\n  Attempting uninstall: bart_score\n    Found existing installation: bart_score 0.1.0\n    Uninstalling bart_score-0.1.0:\n      Successfully uninstalled bart_score-0.1.0\nSuccessfully installed bart_score-0.1.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# # import os\n# import json\n# import csv\n# import pickle\n# from PIL import Image\n# from tqdm import tqdm\n# from sklearn.metrics import accuracy_score, f1_score\n# from nltk.translate.bleu_score import sentence_bleu\n# from rouge_score import rouge_scorer\n# from BARTScore.bart_score import BARTScorer\n# from bert_score import score as bert_score\n# import numpy as np\n# from concurrent.futures import ThreadPoolExecutor, as_completed\n\n# # --- Define Paths ---\n# json_folder_path = \"/kaggle/input/test-dataset/test_dataset\"\n# image_base_path = \"/kaggle/input/abo-small/small\"\n# all_qa_data_path = \"/kaggle/working/all_qa_data1.pkl\"\n# progress_file = \"/kaggle/working/progress1.json\"\n\n# # from your_model_library import model, processor\n\n# # ---- LOAD CACHED DATA OR BUILD ----\n# if os.path.exists(all_qa_data_path):\n#     with open(all_qa_data_path, \"rb\") as f:\n#         all_qa_data = pickle.load(f)\n# else:\n#     print(\"🔄 Processing Q&A data from JSON files...\")\n#     all_qa_data = []\n\n#     def process_json(json_file):\n#         results = []\n#         if not json_file.endswith(\".json\"):\n#             return results\n#         json_path = os.path.join(json_folder_path, json_file)\n#         try:\n#             with open(json_path, \"r\") as f:\n#                 data = json.load(f)\n#         except Exception:\n#             return results\n\n#         raw_path = data[\"image_path\"].replace(\"\\\\\", \"/\")\n#         path_parts = raw_path.split(\"/\")[-2:]\n#         image_rel_path = \"/\".join(path_parts)\n#         full_img_path = os.path.join(image_base_path, image_rel_path)\n\n#         if not os.path.exists(full_img_path):\n#             return results\n\n#         try:\n#             _ = Image.open(full_img_path).convert(\"RGB\")  # preload test\n#         except Exception:\n#             return results\n\n#         for idx, qa_pair in enumerate(data.get(\"qa_pairs\", [])):\n#             question = qa_pair.get(\"question\", \"\").strip()\n#             answer = qa_pair.get(\"answer\", \"\").strip()\n#             if question and answer:\n#                 results.append((json_file, idx, full_img_path, question, answer))\n#         return results\n\n#     with ThreadPoolExecutor() as executor:\n#         futures = [executor.submit(process_json, jf) for jf in os.listdir(json_folder_path)]\n#         for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing JSON files\"):\n#             all_qa_data.extend(future.result())\n\n#     with open(all_qa_data_path, \"wb\") as f:\n#         pickle.dump(all_qa_data, f)\n#     print(\"✅ Q&A data saved to disk.\")\n\n# # ---- LOAD PROGRESS ----\n# if os.path.exists(progress_file):\n#     try:\n#         with open(progress_file, \"r\") as f:\n#             completed_set = set(tuple(x) for x in json.load(f))\n#         print(f\"📘 Resuming from saved progress ({len(completed_set)} Q&A pairs done).\")\n#     except json.JSONDecodeError as e:\n#         print(f\"Error reading progress: {e}\")\n#         completed_set = set()\n# else:\n#     completed_set = set()\n#     print(\"Progress file does not exist. Starting fresh.\")\n\n# # ---- EVALUATION FUNCTION ----\n# def evaluate_qa_pairs(pairs, description, csv_filename):\n#     y_true, y_pred = [], []\n#     all_preds, all_gts = [], []\n#     bleu_scores, rougeL_scores, bart_scores = [], []\n\n#     rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n#     bart_scorer = BARTScorer(device=\"cuda\", checkpoint=\"facebook/bart-large-cnn\")\n\n#     with open(f\"/kaggle/working/{csv_filename}\", 'w', newline='') as csvfile:\n#         fieldnames = ['json_file', 'question_id', 'question', 'ground_truth', 'predicted_answer']\n#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n#         writer.writeheader()\n\n#         for json_file, idx, img_path, question, ground_truth in tqdm(pairs, desc=\"🔍 Evaluating\"):\n#             if (json_file, idx) in completed_set:\n#                 continue\n\n#             try:\n#                 raw_image = Image.open(img_path).convert(\"RGB\")\n#                 inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n#                 out = model.generate(**inputs)\n#                 answer = processor.decode(out[0], skip_special_tokens=True)\n#             except Exception as e:\n#                 print(f\"[ERROR] {json_file} Q{idx+1} → {e}\")\n#                 answer = \"ERROR\"\n\n#             gt = ground_truth.strip().lower()\n#             pred = answer.strip().lower()\n#             y_true.append(gt)\n#             y_pred.append(pred)\n#             all_preds.append(pred)\n#             all_gts.append(gt)\n\n#             bleu_scores.append(sentence_bleu([gt.split()], pred.split()))\n#             rougeL_scores.append(rouge.score(gt, pred)['rougeL'].fmeasure)\n#             bart_scores.append(bart_scorer.score([pred], [gt])[0])\n\n#             writer.writerow({\n#                 'json_file': json_file,\n#                 'question_id': idx,\n#                 'question': question,\n#                 'ground_truth': ground_truth,\n#                 'predicted_answer': answer\n#             })\n\n#             completed_set.add((json_file, idx))\n#             with open(progress_file, \"w\") as f:\n#                 json.dump(list(completed_set), f)\n\n#     # BERTScore (batched)\n#     print(\"📏 Calculating BERTScore...\")\n#     P, R, F1 = bert_score(all_preds, all_gts, lang='en', verbose=True)\n#     bert_P = P.numpy()\n#     bert_R = R.numpy()\n#     bert_F1 = F1.numpy()\n\n#     acc = accuracy_score(y_true, y_pred)\n#     f1 = f1_score(y_true, y_pred, average=\"macro\")\n\n#     print(f\"\\n✅ Evaluation complete: {description}\")\n#     print(f\"Accuracy:       {acc:.4f}\")\n#     print(f\"F1 Score:       {f1:.4f}\")\n#     print(f\"BLEU:           {np.mean(bleu_scores):.4f}\")\n#     print(f\"ROUGE-L:        {np.mean(rougeL_scores):.4f}\")\n#     print(f\"BARTScore:      {np.mean(bart_scores):.4f}\")\n#     print(f\"BERTScore (P/R/F1): {np.mean(bert_P):.4f} / {np.mean(bert_R):.4f} / {np.mean(bert_F1):.4f}\")\n\n# # ---- MAIN EXECUTION ----\n# filtered_qa_data = [item for item in all_qa_data if (item[0], item[1]) not in completed_set]\n# evaluate_qa_pairs(filtered_qa_data, \"Full Dataset\", \"full_dataset_results1.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import csv\n# import pickle\n# from PIL import Image\n# from tqdm import tqdm\n# from sklearn.metrics import accuracy_score, f1_score\n# from nltk.translate.bleu_score import sentence_bleu\n# from rouge_score import rouge_scorer\n# from BARTScore.bart_score import BARTScorer\n# from bert_score import score as bert_score\n# import numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from concurrent.futures import ThreadPoolExecutor, as_completed\n\n# # --- Define Paths ---\n# json_folder_path = \"/kaggle/input/test-dataset/test_dataset\"\n# image_base_path = \"/kaggle/input/abo-small/small\"\n# all_qa_data_path = \"/kaggle/working/all_qa_data1.pkl\"\n# progress_file = \"/kaggle/working/progress1.json\"\n\n# # from your_model_library import model, processor\n\n# # ---- LOAD CACHED DATA OR BUILD ----\n# if os.path.exists(all_qa_data_path):\n#     with open(all_qa_data_path, \"rb\") as f:\n#         all_qa_data = pickle.load(f)\n# else:\n#     print(\"🔄 Processing Q&A data from JSON files...\")\n#     all_qa_data = []\n\n#     def process_json(json_file):\n#         results = []\n#         if not json_file.endswith(\".json\"):\n#             return results\n#         json_path = os.path.join(json_folder_path, json_file)\n#         try:\n#             with open(json_path, \"r\") as f:\n#                 data = json.load(f)\n#         except Exception:\n#             return results\n\n#         raw_path = data[\"image_path\"].replace(\"\\\\\", \"/\")\n#         path_parts = raw_path.split(\"/\")[-2:]\n#         image_rel_path = \"/\".join(path_parts)\n#         full_img_path = os.path.join(image_base_path, image_rel_path)\n\n#         if not os.path.exists(full_img_path):\n#             return results\n\n#         try:\n#             _ = Image.open(full_img_path).convert(\"RGB\")  # preload test\n#         except Exception:\n#             return results\n\n#         for idx, qa_pair in enumerate(data.get(\"qa_pairs\", [])):\n#             question = qa_pair.get(\"question\", \"\").strip()\n#             answer = qa_pair.get(\"answer\", \"\").strip()\n#             if question and answer:\n#                 results.append((json_file, idx, full_img_path, question, answer))\n#         return results\n\n#     with ThreadPoolExecutor() as executor:\n#         futures = [executor.submit(process_json, jf) for jf in os.listdir(json_folder_path)]\n#         for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing JSON files\"):\n#             all_qa_data.extend(future.result())\n\n#     with open(all_qa_data_path, \"wb\") as f:\n#         pickle.dump(all_qa_data, f)\n#     print(\"✅ Q&A data saved to disk.\")\n\n# # ---- LOAD PROGRESS ----\n# if os.path.exists(progress_file):\n#     try:\n#         with open(progress_file, \"r\") as f:\n#             completed_set = set(tuple(x) for x in json.load(f))\n#         print(f\"📘 Resuming from saved progress ({len(completed_set)} Q&A pairs done).\")\n#     except json.JSONDecodeError as e:\n#         print(f\"Error reading progress: {e}\")\n#         completed_set = set()\n# else:\n#     completed_set = set()\n#     print(\"Progress file does not exist. Starting fresh.\")\n\n# # ---- EVALUATION FUNCTION ----\n# def evaluate_qa_pairs(pairs, description, csv_filename):\n#     y_true, y_pred = [], []\n#     all_preds, all_gts = [], []\n#     bleu_scores, rougeL_scores, bart_scores = [], [], []\n\n#     rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n#     bart_scorer = BARTScorer(device=\"cuda\", checkpoint=\"facebook/bart-large-cnn\")\n\n#     with open(f\"/kaggle/working/{csv_filename}\", 'w', newline='') as csvfile:\n#         fieldnames = ['json_file', 'question_id', 'question', 'ground_truth', 'predicted_answer']\n#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n#         writer.writeheader()\n\n#         for json_file, idx, img_path, question, ground_truth in tqdm(pairs, desc=\"🔍 Evaluating\"):\n#             if (json_file, idx) in completed_set:\n#                 continue\n\n#             try:\n#                 raw_image = Image.open(img_path).convert(\"RGB\")\n#                 inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n#                 out = model.generate(**inputs)\n#                 answer = processor.decode(out[0], skip_special_tokens=True)\n#             except Exception as e:\n#                 print(f\"[ERROR] {json_file} Q{idx+1} → {e}\")\n#                 answer = \"ERROR\"\n\n#             gt = ground_truth.strip().lower()\n#             pred = answer.strip().lower()\n#             y_true.append(gt)\n#             y_pred.append(pred)\n#             all_preds.append(pred)\n#             all_gts.append(gt)\n\n#             bleu_scores.append(sentence_bleu([gt.split()], pred.split()))\n#             rougeL_scores.append(rouge.score(gt, pred)['rougeL'].fmeasure)\n#             bart_scores.append(bart_scorer.score([pred], [gt])[0])\n\n#             writer.writerow({\n#                 'json_file': json_file,\n#                 'question_id': idx,\n#                 'question': question,\n#                 'ground_truth': ground_truth,\n#                 'predicted_answer': answer\n#             })\n\n#             completed_set.add((json_file, idx))\n#             with open(progress_file, \"w\") as f:\n#                 json.dump(list(completed_set), f)\n\n#     # BERTScore (batched)\n#     print(\"📏 Calculating BERTScore...\")\n#     P, R, F1 = bert_score(all_preds, all_gts, lang='en', verbose=True)\n#     bert_P = P.numpy()\n#     bert_R = R.numpy()\n#     bert_F1 = F1.numpy()\n\n#     acc = accuracy_score(y_true, y_pred)\n#     f1 = f1_score(y_true, y_pred, average=\"macro\")\n\n#     print(f\"\\n✅ Evaluation complete: {description}\")\n#     print(f\"Accuracy:       {acc:.4f}\")\n#     print(f\"F1 Score:       {f1:.4f}\")\n#     print(f\"BLEU:           {np.mean(bleu_scores):.4f}\")\n#     print(f\"ROUGE-L:        {np.mean(rougeL_scores):.4f}\")\n#     print(f\"BARTScore:      {np.mean(bart_scores):.4f}\")\n#     print(f\"BERTScore (P/R/F1): {np.mean(bert_P):.4f} / {np.mean(bert_R):.4f} / {np.mean(bert_F1):.4f}\")\n\n# # ---- MAIN EXECUTION ----\n# filtered_qa_data = [item for item in all_qa_data if (item[0], item[1]) not in completed_set]\n# evaluate_qa_pairs(filtered_qa_data, \"Full Dataset\", \"full_dataset_results1.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import csv\n# import pickle\n# from PIL import Image\n# from tqdm import tqdm\n# from sklearn.metrics import accuracy_score, f1_score\n# from nltk.translate.bleu_score import sentence_bleu\n# from rouge_score import rouge_scorer\n# from BARTScore.bart_score import BARTScorer\n# from bert_score import score as bert_score\n# import numpy as np\n# import torch\n# from transformers import Blip2Processor, Blip2ForConditionalGeneration\n\n# # --- Define Paths ---\n# json_folder_path = \"/kaggle/input/test-dataset/test_dataset\"\n# image_base_path = \"/kaggle/input/abo-small/small\"\n# all_qa_data_path = \"/kaggle/working/all_qa_data1.pkl\"\n# progress_file = \"/kaggle/working/progress1.json\"\n\n# # # --- Load BLIP-2 ---\n# # processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n# # model = Blip2ForConditionalGeneration.from_pretrained(\n# #     \"Salesforce/blip2-opt-2.7b\",\n# #     device_map=\"auto\",\n# #     torch_dtype=torch.float16\n# # )\n# # model.eval()\n\n# # ---- LOAD OR BUILD DATASET ----\n# if os.path.exists(all_qa_data_path):\n#     with open(all_qa_data_path, \"rb\") as f:\n#         all_qa_data = pickle.load(f)\n# else:\n#     all_qa_data = []\n#     for json_file in tqdm(os.listdir(json_folder_path), desc=\"📂 Loading JSONs\"):\n#         if not json_file.endswith(\".json\"):\n#             continue\n#         try:\n#             with open(os.path.join(json_folder_path, json_file)) as f:\n#                 data = json.load(f)\n#         except:\n#             continue\n\n#         image_rel_path = \"/\".join(data[\"image_path\"].replace(\"\\\\\", \"/\").split(\"/\")[-2:])\n#         full_img_path = os.path.join(image_base_path, image_rel_path)\n#         if not os.path.exists(full_img_path):\n#             continue\n\n#         for idx, qa in enumerate(data.get(\"qa_pairs\", [])):\n#             q, a = qa.get(\"question\", \"\").strip(), qa.get(\"answer\", \"\").strip()\n#             if q and a:\n#                 all_qa_data.append((json_file, idx, full_img_path, q, a))\n\n#     with open(all_qa_data_path, \"wb\") as f:\n#         pickle.dump(all_qa_data, f)\n\n# # ---- LOAD PROGRESS ----\n# completed_set = set()\n# if os.path.exists(progress_file):\n#     try:\n#         with open(progress_file) as f:\n#             completed_set = set(tuple(x) for x in json.load(f))\n#     except:\n#         pass\n\n# # ---- BATCHED EVALUATION ----\n# def evaluate_qa_pairs(pairs, description, csv_filename, batch_size=8):\n#     y_true, y_pred, all_preds, all_gts = [], [], [], []\n#     bleu_scores, rougeL_scores, bart_scores = [], [], []\n\n#     rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n#     bart_scorer = BARTScorer(device=\"cuda\", checkpoint=\"facebook/bart-large-cnn\")\n\n#     csv_path = f\"/kaggle/working/{csv_filename}\"\n#     with open(csv_path, 'a', newline='') as csvfile:\n#         writer = csv.DictWriter(csvfile, fieldnames=[\"json_file\", \"question_id\", \"question\", \"ground_truth\", \"predicted_answer\"])\n#         writer.writeheader()\n\n#         batch = []\n#         for item in tqdm(pairs, desc=\"⚡ Batched Evaluation\"):\n#             if (item[0], item[1]) in completed_set:\n#                 continue\n#             batch.append(item)\n#             if len(batch) == batch_size:\n#                 process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n#                               bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer)\n#                 batch = []\n#         if batch:\n#             process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n#                           bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer)\n\n#     # Save progress\n#     with open(progress_file, \"w\") as f:\n#         json.dump(list(completed_set), f)\n\n#     # BERTScore (batch)\n#     print(\"📏 Calculating BERTScore...\")\n#     P, R, F1 = bert_score(all_preds, all_gts, lang='en', verbose=True)\n#     acc = accuracy_score(y_true, y_pred)\n#     f1 = f1_score(y_true, y_pred, average=\"macro\")\n\n#     print(f\"\\n✅ {description} Complete\")\n#     print(f\"Accuracy:        {acc:.4f}\")\n#     print(f\"F1 Score:        {f1:.4f}\")\n#     print(f\"BLEU:            {np.mean(bleu_scores):.4f}\")\n#     print(f\"ROUGE-L:         {np.mean(rougeL_scores):.4f}\")\n#     print(f\"BARTScore:       {np.mean(bart_scores):.4f}\")\n#     print(f\"BERTScore P/R/F: {P.mean():.4f} / {R.mean():.4f} / {F1.mean():.4f}\")\n\n# def process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n#                   bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer):\n#     images, questions, metas = [], [], []\n#     for json_file, idx, img_path, q, a in batch:\n#         try:\n#             img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n#         except:\n#             continue\n#         images.append(img)\n#         questions.append(q)\n#         metas.append((json_file, idx, q, a))\n\n#     if not images:\n#         return\n\n#     inputs = processor(images=images, text=questions, return_tensors=\"pt\", padding=True).to(\"cuda\")\n#     with torch.inference_mode():\n#         outputs = model.generate(**inputs, max_new_tokens=30)\n#     answers = processor.batch_decode(outputs, skip_special_tokens=True)\n\n#     for ans, (json_file, idx, question, gt) in zip(answers, metas):\n#         pred = ans.strip().lower()\n#         gt_clean = gt.strip().lower()\n#         all_preds.append(pred)\n#         all_gts.append(gt_clean)\n#         y_true.append(gt_clean)\n#         y_pred.append(pred)\n\n#         bleu_scores.append(sentence_bleu([gt_clean.split()], pred.split()))\n#         rougeL_scores.append(rouge.score(gt_clean, pred)['rougeL'].fmeasure)\n#         bart_scores.append(bart_scorer.score([pred], [gt_clean])[0])\n\n#         writer.writerow({\n#             \"json_file\": json_file,\n#             \"question_id\": idx,\n#             \"question\": question,\n#             \"ground_truth\": gt,\n#             \"predicted_answer\": ans\n#         })\n#         completed_set.add((json_file, idx))\n\n# # ---- MAIN EXECUTION ----\n# filtered_qa_data = [item for item in all_qa_data if (item[0], item[1]) not in completed_set]\n# evaluate_qa_pairs(filtered_qa_data, \"Full Dataset\", \"full_dataset_results1.csv\", batch_size=8)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import csv\n# from bert_score import score as bert_score\n# import numpy as np\n\n# # --- CONFIG ---\n# csv_path = \"/kaggle/working/full_dataset_results.csv\"  # or any result file\n# bert_lang = \"en\"  # language for BERTScore\n\n# # --- LOAD CSV DATA ---\n# ground_truths = []\n# predictions = []\n\n# print(f\"📂 Reading predictions from {csv_path}...\")\n# with open(csv_path, 'r') as f:\n#     reader = csv.DictReader(f)\n#     for row in reader:\n#         gt = row['ground_truth'].strip()\n#         pred = row['predicted_answer'].strip()\n#         if gt and pred:\n#             ground_truths.append(gt)\n#             predictions.append(pred)\n\n# print(f\"✅ Loaded {len(ground_truths)} Q&A pairs.\")\n\n# # --- COMPUTE BERTSCORE ---\n# print(\"🔍 Computing BERTScore...\")\n# P, R, F1 = bert_score(predictions, ground_truths, lang=bert_lang, verbose=True)\n# P = P.numpy()\n# R = R.numpy()\n# F1 = F1.numpy()\n# # --- OUTPUT RESULTS ---\n# print(\"\\n🎯 BERTScore Results:\")\n# print(f\"Precision:  {np.mean(P):.4f}\")\n# print(f\"Recall:     {np.mean(R):.4f}\")\n# print(f\"F1 Score:   {np.mean(F1):.4f}\")\n\n# # --- OPTIONAL: SAVE DETAILED SCORES ---\n# with open(\"/kaggle/working/bertscore_details.csv\", \"w\", newline=\"\") as f:\n#     writer = csv.writer(f)\n#     writer.writerow([\"Ground Truth\", \"Prediction\", \"Precision\", \"Recall\", \"F1\"])\n#     for gt, pred, p, r, f1 in zip(ground_truths, predictions, P, R, F1):\n#         writer.writerow([gt, pred, f\"{p:.4f}\", f\"{r:.4f}\", f\"{f1:.4f}\"])\n\n# print(\"📁 Saved detailed BERTScore results to bertscore_details.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T07:40:34.537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport csv\nimport pickle\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nfrom BARTScore.bart_score import BARTScorer\nfrom bert_score import score as bert_score\nimport numpy as np\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n# --- Define Paths ---\njson_folder_path = \"/kaggle/input/master-test/test_dataset\"\nimage_base_path = \"/kaggle/input/abo-small/small\"\nall_qa_data_path = \"/kaggle/working/all_qa_data1.pkl\"\nprogress_file = \"/kaggle/working/progress1.json\"\ncsv_path = \"/kaggle/working/full_dataset_results1.csv\"\n\n# --- Load BLIP-2 ---\n# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n# model = Blip2ForConditionalGeneration.from_pretrained(\n#     \"Salesforce/blip2-opt-2.7b\",\n#     device_map=\"auto\",\n#     torch_dtype=torch.float16\n# )\n# model.eval()\n\n# ---- LOAD OR BUILD DATASET ----\nif os.path.exists(all_qa_data_path):\n    with open(all_qa_data_path, \"rb\") as f:\n        all_qa_data = pickle.load(f)\nelse:\n    all_qa_data = []\n    for json_file in tqdm(os.listdir(json_folder_path), desc=\"📂 Loading JSONs\"):\n        if not json_file.endswith(\".json\"):\n            continue\n        try:\n            with open(os.path.join(json_folder_path, json_file)) as f:\n                data = json.load(f)\n        except:\n            continue\n\n        image_rel_path = \"/\".join(data[\"image_path\"].replace(\"\\\\\", \"/\").split(\"/\")[-2:])\n        full_img_path = os.path.join(image_base_path, image_rel_path)\n        if not os.path.exists(full_img_path):\n            continue\n\n        for idx, qa in enumerate(data.get(\"qa_pairs\", [])):\n            q, a = qa.get(\"question\", \"\").strip(), qa.get(\"answer\", \"\").strip()\n            if q and a:\n                all_qa_data.append((json_file, idx, full_img_path, q, a))\n\n    with open(all_qa_data_path, \"wb\") as f:\n        pickle.dump(all_qa_data, f)\n\n# ---- LOAD PROGRESS ----\ncompleted_set = set()\nif os.path.exists(progress_file):\n    try:\n        with open(progress_file) as f:\n            completed_set = set(tuple(x) for x in json.load(f))\n    except:\n        pass\n\n# ---- BATCH PROCESSING UTILS ----\ndef load_image_resize(path):\n    try:\n        return Image.open(path).convert(\"RGB\").resize((224, 224))\n    except:\n        return None\n\ndef process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n                  bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer):\n    metas = [(json_file, idx, img_path, q, a) for json_file, idx, img_path, q, a in batch]\n    questions = [q for _, _, _, q, _ in metas]\n\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        images = list(executor.map(lambda x: load_image_resize(x[2]), metas))\n\n    valid_data = [(img, meta) for img, meta in zip(images, metas) if img is not None]\n    if not valid_data:\n        return\n\n    images = [img for img, _ in valid_data]\n    metas = [meta for _, meta in valid_data]\n    questions = [q for _, _, _, q, _ in metas]\n\n    inputs = processor(images=images, text=questions, return_tensors=\"pt\", padding=True).to(\"cuda\")\n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=30)\n    answers = processor.batch_decode(outputs, skip_special_tokens=True)\n\n    for ans, (json_file, idx, _, question, gt) in zip(answers, metas):\n        pred = ans.strip().lower()\n        gt_clean = gt.strip().lower()\n        all_preds.append(pred)\n        all_gts.append(gt_clean)\n        y_true.append(gt_clean)\n        y_pred.append(pred)\n    \n        bleu_scores.append(sentence_bleu([gt_clean.split()], pred.split()))\n        rougeL_scores.append(rouge.score(gt_clean, pred)['rougeL'].fmeasure)\n        bart_scores.append(bart_scorer.score([pred], [gt_clean])[0])\n    \n        writer.writerow({\n            \"json_file\": json_file,\n            \"question_id\": idx,\n            \"question\": question,\n            \"ground_truth\": gt,\n            \"predicted_answer\": ans\n        })\n        completed_set.add((json_file, idx))\n\n    # Save progress after each batch\n    with open(progress_file, \"w\") as f:\n        json.dump(list(completed_set), f)\n\n# ---- MAIN EVALUATION FUNCTION ----\ndef evaluate_qa_pairs(pairs, description, batch_size=64):\n    y_true, y_pred, all_preds, all_gts = [], [], [], []\n    bleu_scores, rougeL_scores, bart_scores = [], [], []\n\n    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    bart_scorer = BARTScorer(device=\"cuda\", checkpoint=\"facebook/bart-large-cnn\")\n\n    write_header = not os.path.exists(csv_path)\n    with open(csv_path, 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=[\"json_file\", \"question_id\", \"question\", \"ground_truth\", \"predicted_answer\"])\n        if write_header:\n            writer.writeheader()\n    \n        batch = []\n        with tqdm(total=len(pairs), desc=\"⚡ Batched Evaluation\", dynamic_ncols=True) as pbar:\n            for item in pairs:\n                if (item[0], item[1]) in completed_set:\n                    pbar.update(1)  # Update progress bar by 1 for skipped item\n                    continue\n                batch.append(item)\n                if len(batch) == batch_size:\n                    process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n                                  bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer)\n                    pbar.update(len(batch))  # Update progress after processing a full batch\n                    batch = []\n            \n            # Process any remaining items in the batch\n            if batch:\n                process_batch(batch, writer, y_true, y_pred, all_preds, all_gts,\n                              bleu_scores, rougeL_scores, bart_scores, rouge, bart_scorer)\n                pbar.update(len(batch))  # Update progress after the last batch\n    \n            pbar.close()  # Close the progress bar after processing all batches\n\n\n\n    print(\"📏 Calculating BERTScore...\")\n    P, R, F1 = bert_score(all_preds, all_gts, lang='en', verbose=True)\n    P = P.numpy()\n    R = R.numpy()\n    F1 = F1.numpy()\n    \n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average=\"macro\")\n\n    print(f\"\\n✅ {description} Complete\")\n    print(f\"Accuracy:        {acc:.4f}\")\n    print(f\"F1 Score:        {f1:.4f}\")\n    print(f\"BLEU:            {np.mean(bleu_scores):.4f}\")\n    print(f\"ROUGE-L:         {np.mean(rougeL_scores):.4f}\")\n    print(f\"BARTScore:       {np.mean(bart_scores):.4f}\")\n    print(f\"BERTScore P/R/F: {P.mean():.4f} / {R.mean():.4f} / {F1.mean():.4f}\")\n\n# ---- EXECUTION ----\nfiltered_qa_data = [item for item in all_qa_data if (item[0], item[1]) not in completed_set]\nevaluate_qa_pairs(filtered_qa_data, \"Full Dataset\", batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T07:59:01.772598Z","iopub.execute_input":"2025-05-07T07:59:01.773210Z","iopub.status.idle":"2025-05-07T07:59:05.192575Z","shell.execute_reply.started":"2025-05-07T07:59:01.773179Z","shell.execute_reply":"2025-05-07T07:59:05.191521Z"}},"outputs":[{"name":"stderr","text":"⚡ Batched Evaluation: 0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"📏 Calculating BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_60430/3356460687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;31m# ---- EXECUTION ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0mfiltered_qa_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_qa_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompleted_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mevaluate_qa_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_qa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Full Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_60430/3356460687.py\u001b[0m in \u001b[0;36mevaluate_qa_pairs\u001b[0;34m(pairs, description, batch_size)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📏 Calculating BERTScore...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_gts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bert_score/score.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mref_group_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mref_group_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mori_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\nfrom bert_score import score as bert_score\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Load your CSV\ndf = pd.read_csv(\"/kaggle/working/full_dataset_results1.csv\")\ndf.columns = df.columns.str.strip().str.lower()  # normalize column names\n\n# Extract predictions and ground truths\nall_preds = df['predicted_answer'].astype(str).str.strip().str.lower().tolist()\nall_gts = df['ground_truth'].astype(str).str.strip().str.lower().tolist()\n\n\n# 🎯 Exact Match Accuracy (using sklearn)\nacc = accuracy_score(all_gts, all_preds)\nf1_exact = f1_score(all_gts, all_preds, average='macro')  # or 'weighted'\n\nprint(f\"✅ Accuracy (sklearn exact match): {acc:.4f}\")\nprint(f\"🎯 F1 Score (exact match, macro): {f1_exact:.4f}\")\n\n# 🧠 BERTScore (semantic similarity)\nprint(\"📏 Calculating BERTScore...\")\nP, R, F1 = bert_score(all_preds, all_gts, lang='en', verbose=True)\nprint(f\"\\n🔍 BERTScore (avg):\\nPrecision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:09:59.337773Z","iopub.execute_input":"2025-05-07T08:09:59.338117Z","iopub.status.idle":"2025-05-07T08:12:17.075015Z","shell.execute_reply.started":"2025-05-07T08:09:59.338059Z","shell.execute_reply":"2025-05-07T08:12:17.074134Z"}},"outputs":[{"name":"stdout","text":"✅ Accuracy (sklearn exact match): 0.1768\n🎯 F1 Score (exact match, macro): 0.0076\n📏 Calculating BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/557 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2228405a134945dd93bbf0f9f5a1feec"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7532 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67ef5a7556f4435af6da37c7b37040b"}},"metadata":{}},{"name":"stdout","text":"done in 120.20 seconds, 4010.23 sentences/sec\n\n🔍 BERTScore (avg):\nPrecision: 0.9408, Recall: 0.9310, F1: 0.9350\n","output_type":"stream"}],"execution_count":22}]}